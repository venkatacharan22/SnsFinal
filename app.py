from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import google.generativeai as genai
import os
import json
import base64
from PIL import Image
import io
import pickle
import numpy as np
import pandas as pd
from gtts import gTTS
import tempfile
import os
import re
import string
from sklearn.feature_extraction.text import TfidfVectorizer
import pytesseract
import cv2

app = Flask(__name__)
CORS(app, origins=[
    "http://localhost:5001",
    "http://localhost:5002",
    "https://sns01-a8fba.web.app",
    "https://sns01-a8fba.firebaseapp.com",
    "https://storage.googleapis.com"
])

# Configure Gemini API for explanations
genai.configure(api_key="AIzaSyCBv8jNE-5K8Ojs0UumdeBL_Zba68b4e18")
gemini_model = genai.GenerativeModel('gemini-2.0-flash-exp')

# Load trained models
print("Loading ML models...")
try:
    with open('fake_news_model.pkl', 'rb') as f:
        fake_news_model = pickle.load(f)
    with open('tfidf_vectorizer.pkl', 'rb') as f:
        tfidf_vectorizer = pickle.load(f)
    print("‚úÖ ML models loaded successfully!")
except Exception as e:
    print(f"‚ùå Error loading models: {e}")
    fake_news_model = None
    tfidf_vectorizer = None

# Language mappings
LANGUAGES = {
    'en': 'English',
    'hi': 'Hindi',
    'ta': 'Tamil',
    'te': 'Telugu',
    'ml': 'Malayalam',
    'kn': 'Kannada',
    'bn': 'Bengali',
    'gu': 'Gujarati',
    'mr': 'Marathi',
    'pa': 'Punjabi',
    'es': 'Spanish',
    'fr': 'French',
    'de': 'German',
    'it': 'Italian',
    'pt': 'Portuguese',
    'ru': 'Russian',
    'ja': 'Japanese',
    'ko': 'Korean',
    'zh': 'Chinese',
    'ar': 'Arabic',
    'tr': 'Turkish',
    'nl': 'Dutch',
    'sv': 'Swedish'
}

# Text preprocessing function
def preprocess_text(text):
    """Clean and preprocess text for model prediction"""
    if not text:
        return ""

    # Convert to lowercase
    text = text.lower()

    # Remove special characters and digits
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # Remove extra whitespace
    text = ' '.join(text.split())

    return text

# OCR function for image text extraction
def extract_text_from_image(image):
    """Extract text from image using OCR"""
    try:
        # Convert PIL image to numpy array for OpenCV
        img_array = np.array(image)

        # Convert RGB to BGR for OpenCV
        if len(img_array.shape) == 3:
            img_array = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)

        # Use pytesseract to extract text
        extracted_text = pytesseract.image_to_string(img_array)

        return extracted_text.strip()
    except Exception as e:
        print(f"OCR Error: {e}")
        return ""

# ML Model prediction function
def predict_fake_news(text):
    """Predict if news is fake using trained ML model"""
    try:
        if not fake_news_model or not tfidf_vectorizer:
            return None, 0.5

        # Preprocess text
        cleaned_text = preprocess_text(text)

        if not cleaned_text:
            return None, 0.5

        # Vectorize text using trained TF-IDF vectorizer
        text_vectorized = tfidf_vectorizer.transform([cleaned_text])

        # Get prediction and probability
        prediction = fake_news_model.predict(text_vectorized)[0]
        prediction_proba = fake_news_model.predict_proba(text_vectorized)[0]

        # Get confidence (probability of the predicted class)
        confidence = max(prediction_proba)

        # Debug logging
        print(f"Raw ML prediction: {prediction}")
        print(f"Prediction probabilities: {prediction_proba}")
        print(f"Confidence: {confidence}")

        # Convert prediction to boolean
        # Standard interpretation: 0 = real, 1 = fake (most common in sklearn)
        # Let's test both interpretations and use the one that makes more sense
        is_fake = bool(prediction)  # Standard interpretation

        # If confidence is low, adjust the prediction to be more balanced
        if confidence < 0.6:
            # For low confidence, make it more balanced based on content analysis
            fake_indicators = ['shocking', 'breaking', 'unbelievable', 'secret', 'exposed', 'doctors hate', 'miracle']
            real_indicators = ['according to', 'study shows', 'research', 'published', 'scientists', 'official']

            text_lower = cleaned_text.lower()
            fake_score = sum(1 for indicator in fake_indicators if indicator in text_lower)
            real_score = sum(1 for indicator in real_indicators if indicator in text_lower)

            print(f"Content analysis - Fake indicators: {fake_score}, Real indicators: {real_score}")

            # Adjust prediction based on content analysis
            if fake_score > real_score:
                is_fake = True
                confidence = min(0.85, confidence + 0.1)
            elif real_score > fake_score:
                is_fake = False
                confidence = min(0.85, confidence + 0.1)
            # If equal, keep original prediction but boost confidence slightly
            else:
                confidence = min(0.8, confidence + 0.05)

        print(f"Final prediction: {'FAKE' if is_fake else 'REAL'} with {confidence:.2f} confidence")
        return is_fake, confidence

    except Exception as e:
        print(f"ML Prediction Error: {e}")
        return None, 0.5

# Gemini explanation function
def get_gemini_explanation(text, is_fake, confidence, language='en'):
    """Get explanation from Gemini AI about why news is fake/real in the specified language"""
    try:
        # Enhanced language mapping with native names
        language_names = {
            'hi': 'Hindi (‡§π‡§ø‡§Ç‡§¶‡•Ä)',
            'ta': 'Tamil (‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç)',
            'te': 'Telugu (‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å)',
            'ml': 'Malayalam (‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç)',
            'kn': 'Kannada (‡≤ï‡≤®‡≥ç‡≤®‡≤°)',
            'bn': 'Bengali (‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ)',
            'gu': 'Gujarati (‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä)',
            'mr': 'Marathi (‡§Æ‡§∞‡§æ‡§†‡•Ä)',
            'pa': 'Punjabi (‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä)',
            'en': 'English',
            'es': 'Spanish (Espa√±ol)',
            'fr': 'French (Fran√ßais)',
            'de': 'German (Deutsch)',
            'it': 'Italian (Italiano)',
            'pt': 'Portuguese (Portugu√™s)',
            'ru': 'Russian (–†—É—Å—Å–∫–∏–π)',
            'ja': 'Japanese (Êó•Êú¨Ë™û)',
            'ko': 'Korean (ÌïúÍµ≠Ïñ¥)',
            'zh': 'Chinese (‰∏≠Êñá)',
            'ar': 'Arabic (ÿßŸÑÿπÿ±ÿ®Ÿäÿ©)',
            'tr': 'Turkish (T√ºrk√ße)',
            'nl': 'Dutch (Nederlands)',
            'sv': 'Swedish (Svenska)'
        }

        status = "FAKE" if is_fake else "REAL"
        language_name = language_names.get(language, 'English')

        prompt = f"""
        You are an expert multilingual fact-checker. A machine learning model has analyzed news content and classified it as {status} with {confidence:.1%} confidence.

        News Text: "{text}"

        CRITICAL: Respond ENTIRELY in {language_name}. Use native script where applicable (Devanagari for Hindi, Tamil script for Tamil, etc.).

        Provide a comprehensive explanation covering:

        1. **ML Analysis**: Why the model classified this as {status.lower()}
        2. **Key Indicators**: Specific patterns that led to this classification
        3. **Verification Tips**: How readers can fact-check such content
        4. **Cultural Context**: Misinformation patterns relevant to {language_name} speakers
        5. **Action Steps**: What readers should do next

        Requirements:
        - Write ONLY in {language_name}
        - Use appropriate cultural context
        - Be educational and helpful
        - Explain in simple, clear terms
        - Provide practical advice

        Format as a flowing, educational explanation that empowers users to be better news consumers.
        """

        response = gemini_model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.2,  # Lower for more consistent multilingual output
                max_output_tokens=1500  # More tokens for detailed explanations
            )
        )

        return response.text

    except Exception as e:
        print(f"Gemini Explanation Error: {e}")
        # Provide fallback explanations in the target language
        fallback_explanations = {
            'hi': f"‡§Æ‡§∂‡•Ä‡§® ‡§≤‡§∞‡•ç‡§®‡§ø‡§Ç‡§ó ‡§Æ‡•â‡§°‡§≤ ‡§®‡•á ‡§á‡§∏ ‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞ ‡§ï‡•ã {confidence:.1%} ‡§µ‡§ø‡§∂‡•ç‡§µ‡§æ‡§∏ ‡§ï‡•á ‡§∏‡§æ‡§• {status} ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡§∞‡•ç‡§ó‡•Ä‡§ï‡•É‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§ï‡§à ‡§µ‡§ø‡§∂‡•ç‡§µ‡§∏‡§®‡•Ä‡§Ø ‡§∏‡•ç‡§∞‡•ã‡§§‡•ã‡§Ç ‡§∏‡•á ‡§∏‡§§‡•ç‡§Ø‡§æ‡§™‡§® ‡§ï‡§∞‡•á‡§Ç‡•§",
            'ta': f"‡Æá‡ÆØ‡Æ®‡Øç‡Æ§‡Æø‡Æ∞ ‡Æï‡Æ±‡Øç‡Æ±‡Æ≤‡Øç ‡ÆÆ‡Ææ‡Æ§‡Æø‡Æ∞‡Æø ‡Æá‡Æ®‡Øç‡Æ§ ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡Æø‡ÆØ‡Øà {confidence:.1%} ‡Æ®‡ÆÆ‡Øç‡Æ™‡Æø‡Æï‡Øç‡Æï‡Øà‡ÆØ‡ØÅ‡Æü‡Æ©‡Øç {status} ‡Æé‡Æ© ‡Æµ‡Æï‡Øà‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡Æø‡ÆØ‡ØÅ‡Æ≥‡Øç‡Æ≥‡Æ§‡ØÅ‡•§ ‡Æ™‡Æ≤ ‡Æ®‡ÆÆ‡Øç‡Æ™‡Æï‡ÆÆ‡Ææ‡Æ© ‡ÆÜ‡Æ§‡Ææ‡Æ∞‡Æô‡Øç‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡Æá‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡ØÅ ‡Æö‡Æ∞‡Æø‡Æ™‡Ææ‡Æ∞‡Øç‡Æï‡Øç‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç‡•§",
            'te': f"‡∞Æ‡±Ü‡∞∑‡∞ø‡∞®‡±ç ‡∞≤‡±Ü‡∞∞‡±ç‡∞®‡∞ø‡∞Ç‡∞ó‡±ç ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞à ‡∞µ‡∞æ‡∞∞‡±ç‡∞§‡∞®‡±Å {confidence:.1%} ‡∞µ‡∞ø‡∞∂‡±ç‡∞µ‡∞æ‡∞∏‡∞Ç‡∞§‡±ã {status} ‡∞ó‡∞æ ‡∞µ‡∞∞‡±ç‡∞ó‡±Ä‡∞ï‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø‡∞Ç‡∞¶‡∞ø‡•§ ‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞Ö‡∞®‡±á‡∞ï ‡∞µ‡∞ø‡∞∂‡±ç‡∞µ‡∞∏‡∞®‡±Ä‡∞Ø ‡∞Æ‡±Ç‡∞≤‡∞æ‡∞≤ ‡∞®‡±Å‡∞Ç‡∞°‡∞ø ‡∞ß‡±É‡∞µ‡±Ä‡∞ï‡∞∞‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø‡•§",
            'ml': f"‡¥Æ‡µÜ‡¥∑‡µÄ‡µª ‡¥≤‡µá‡¥£‡¥ø‡¥Ç‡¥ó‡µç ‡¥Æ‡µã‡¥°‡µΩ ‡¥à ‡¥µ‡¥æ‡µº‡¥§‡µç‡¥§‡¥Ø‡µÜ {confidence:.1%} ‡¥µ‡¥ø‡¥∂‡µç‡¥µ‡¥æ‡¥∏‡¥§‡µç‡¥§‡µã‡¥ü‡µÜ {status} ‡¥Ü‡¥Ø‡¥ø ‡¥§‡¥∞‡¥Ç‡¥§‡¥ø‡¥∞‡¥ø‡¥ö‡µç‡¥ö‡¥ø‡¥ü‡µç‡¥ü‡µÅ‡¥£‡µç‡¥ü‡µç. ‡¥¶‡¥Ø‡¥µ‡¥æ‡¥Ø‡¥ø ‡¥í‡¥®‡µç‡¥®‡¥ø‡¥≤‡¥ß‡¥ø‡¥ï‡¥Ç ‡¥µ‡¥ø‡¥∂‡µç‡¥µ‡¥∏‡¥®‡µÄ‡¥Ø ‡¥∏‡µç‡¥∞‡µã‡¥§‡¥∏‡µç‡¥∏‡µÅ‡¥ï‡¥≥‡¥ø‡µΩ ‡¥®‡¥ø‡¥®‡µç‡¥®‡µç ‡¥™‡¥∞‡¥ø‡¥∂‡µã‡¥ß‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï‡•§",
            'kn': f"‡≤Ø‡≤Ç‡≤§‡≥ç‡≤∞ ‡≤ï‡≤≤‡≤ø‡≤ï‡≥Ü ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤Ø‡≥Å ‡≤à ‡≤∏‡≥Å‡≤¶‡≥ç‡≤¶‡≤ø‡≤Ø‡≤®‡≥ç‡≤®‡≥Å {confidence:.1%} ‡≤µ‡≤ø‡≤∂‡≥ç‡≤µ‡≤æ‡≤∏‡≤¶‡≥ä‡≤Ç‡≤¶‡≤ø‡≤ó‡≥Ü {status} ‡≤é‡≤Ç‡≤¶‡≥Å ‡≤µ‡≤∞‡≥ç‡≤ó‡≥Ä‡≤ï‡≤∞‡≤ø‡≤∏‡≤ø‡≤¶‡≥Ü. ‡≤¶‡≤Ø‡≤µ‡≤ø‡≤ü‡≥ç‡≤ü‡≥Å ‡≤Ö‡≤®‡≥á‡≤ï ‡≤µ‡≤ø‡≤∂‡≥ç‡≤µ‡≤æ‡≤∏‡≤æ‡≤∞‡≥ç‡≤π ‡≤Æ‡≥Ç‡≤≤‡≤ó‡≤≥‡≤ø‡≤Ç‡≤¶ ‡≤™‡≤∞‡≤ø‡≤∂‡≥Ä‡≤≤‡≤ø‡≤∏‡≤ø‡•§",
            'en': f"The machine learning model classified this news as {status} with {confidence:.1%} confidence. Please verify through multiple reliable sources."
        }
        return fallback_explanations.get(language, fallback_explanations['en'])

def get_multilingual_content(language, is_fake, confidence):
    """Get multilingual content for sources, recommendations, etc."""

    # Multilingual content dictionary
    content = {
        'hi': {
            'sources': [
                {"name": "‡§è‡§Æ‡§è‡§≤ ‡§Æ‡•â‡§°‡§≤ ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§£", "credibility": "‡§â‡§ö‡•ç‡§ö"},
                {"name": "‡§ü‡•Ä‡§è‡§´-‡§Ü‡§à‡§°‡•Ä‡§è‡§´ ‡§µ‡•á‡§ï‡•ç‡§ü‡§∞‡§æ‡§á‡§ú‡§º‡•á‡§∂‡§®", "credibility": "‡§â‡§ö‡•ç‡§ö"}
            ],
            'red_flags': [
                "‡§∏‡§Ç‡§¶‡§ø‡§ó‡•ç‡§ß ‡§≠‡§æ‡§∑‡§æ ‡§™‡•à‡§ü‡§∞‡•ç‡§® ‡§ï‡§æ ‡§™‡§§‡§æ ‡§ö‡§≤‡§æ",
                "‡§∏‡§æ‡§Æ‡§ó‡•ç‡§∞‡•Ä ‡§ï‡•Ä ‡§µ‡§ø‡§∂‡•á‡§∑‡§§‡§æ‡§è‡§Ç ‡§´‡•á‡§ï ‡§®‡•ç‡§Ø‡•Ç‡§ú‡§º ‡§ü‡•ç‡§∞‡•á‡§®‡§ø‡§Ç‡§ó ‡§°‡•á‡§ü‡§æ ‡§∏‡•á ‡§Æ‡•á‡§≤ ‡§ñ‡§æ‡§§‡•Ä ‡§π‡•à‡§Ç",
                "‡§ó‡§≤‡§§ ‡§∏‡•Ç‡§ö‡§®‡§æ ‡§ï‡•Ä ‡§â‡§ö‡•ç‡§ö ‡§∏‡§Ç‡§≠‡§æ‡§µ‡§®‡§æ"
            ],
            'recommendation_real': "‡§∏‡§æ‡§Æ‡§ó‡•ç‡§∞‡•Ä ‡§µ‡§ø‡§∂‡•ç‡§µ‡§∏‡§®‡•Ä‡§Ø ‡§≤‡§ó‡§§‡•Ä ‡§π‡•à, ‡§≤‡•á‡§ï‡§ø‡§® ‡§∏‡§æ‡§ù‡§æ ‡§ï‡§∞‡§®‡•á ‡§∏‡•á ‡§™‡§π‡§≤‡•á ‡§π‡§Æ‡•á‡§∂‡§æ ‡§ï‡§à ‡§µ‡§ø‡§∂‡•ç‡§µ‡§∏‡§®‡•Ä‡§Ø ‡§∏‡•ç‡§∞‡•ã‡§§‡•ã‡§Ç ‡§∏‡•á ‡§∏‡§§‡•ç‡§Ø‡§æ‡§™‡§® ‡§ï‡§∞‡•á‡§Ç‡•§",
            'recommendation_fake': "‡§á‡§∏ ‡§∏‡§æ‡§Æ‡§ó‡•ç‡§∞‡•Ä ‡§ï‡•á ‡§∏‡§æ‡§• ‡§∏‡§æ‡§µ‡§ß‡§æ‡§®‡•Ä ‡§¨‡§∞‡§§‡•á‡§Ç‡•§ ‡§∏‡§æ‡§ù‡§æ ‡§ï‡§∞‡§®‡•á ‡§∏‡•á ‡§™‡§π‡§≤‡•á ‡§ï‡§à ‡§µ‡§ø‡§∂‡•ç‡§µ‡§∏‡§®‡•Ä‡§Ø ‡§∏‡§Æ‡§æ‡§ö‡§æ‡§∞ ‡§∏‡•ç‡§∞‡•ã‡§§‡•ã‡§Ç ‡§∏‡•á ‡§∏‡§§‡•ç‡§Ø‡§æ‡§™‡§® ‡§ï‡§∞‡•á‡§Ç‡•§",
            'debunked_by': ["‡§è‡§Æ‡§è‡§≤ ‡§Æ‡•â‡§°‡§≤ ‡§µ‡§∞‡•ç‡§ó‡•Ä‡§ï‡§∞‡§£"],
            'summary': f"‡§è‡§Æ‡§è‡§≤ ‡§Æ‡•â‡§°‡§≤ ‡§®‡•á {confidence}% ‡§µ‡§ø‡§∂‡•ç‡§µ‡§æ‡§∏ ‡§ï‡•á ‡§∏‡§æ‡§• {'‡§´‡•á‡§ï' if is_fake else '‡§∞‡§ø‡§Ø‡§≤'} ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç ‡§µ‡§∞‡•ç‡§ó‡•Ä‡§ï‡•É‡§§ ‡§ï‡§ø‡§Ø‡§æ",
            'model_name': "‡§™‡•ç‡§∞‡§∂‡§ø‡§ï‡•ç‡§∑‡§ø‡§§ ‡§ï‡§æ‡§ó‡§≤ ‡§Æ‡•â‡§°‡§≤"
        },
        'ta': {
            'sources': [
                {"name": "ML ‡ÆÆ‡Ææ‡Æ§‡Æø‡Æ∞‡Æø ‡Æ™‡Æï‡ØÅ‡Æ™‡Øç‡Æ™‡Ææ‡ÆØ‡Øç‡Æµ‡ØÅ", "credibility": "‡Æâ‡ÆØ‡Æ∞‡Øç"},
                {"name": "TF-IDF ‡Æµ‡ØÜ‡Æï‡Øç‡Æü‡Æ∞‡Øà‡Æö‡Øá‡Æ∑‡Æ©‡Øç", "credibility": "‡Æâ‡ÆØ‡Æ∞‡Øç"}
            ],
            'red_flags': [
                "‡Æö‡Æ®‡Øç‡Æ§‡Øá‡Æï‡Æ§‡Øç‡Æ§‡Æø‡Æ±‡Øç‡Æï‡ØÅ‡Æ∞‡Æø‡ÆØ ‡ÆÆ‡Øä‡Æ¥‡Æø ‡Æµ‡Æü‡Æø‡Æµ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡ÆØ‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡Æ©",
                "‡Æâ‡Æ≥‡Øç‡Æ≥‡Æü‡Æï‡Øç‡Æï ‡Æ™‡Æ£‡Øç‡Æ™‡ØÅ‡Æï‡Æ≥‡Øç ‡Æ™‡Øã‡Æ≤‡Æø ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡Æø ‡Æ™‡ÆØ‡Æø‡Æ±‡Øç‡Æö‡Æø ‡Æ§‡Æ∞‡Æµ‡ØÅ‡Æü‡Æ©‡Øç ‡Æ™‡Øä‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡ØÅ‡Æï‡Æø‡Æ©‡Øç‡Æ±‡Æ©",
                "‡Æ§‡Æµ‡Æ±‡Ææ‡Æ© ‡Æ§‡Æï‡Æµ‡Æ≤‡Æø‡Æ©‡Øç ‡ÆÖ‡Æ§‡Æø‡Æï ‡Æµ‡Ææ‡ÆØ‡Øç‡Æ™‡Øç‡Æ™‡ØÅ"
            ],
            'recommendation_real': "‡Æâ‡Æ≥‡Øç‡Æ≥‡Æü‡Æï‡Øç‡Æï‡ÆÆ‡Øç ‡Æ®‡ÆÆ‡Øç‡Æ™‡Æï‡ÆÆ‡Ææ‡Æ©‡Æ§‡Ææ‡Æï‡Æ§‡Øç ‡Æ§‡Øã‡Æ©‡Øç‡Æ±‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ, ‡ÆÜ‡Æ©‡Ææ‡Æ≤‡Øç ‡Æ™‡Æï‡Æø‡Æ∞‡Øç‡Æµ‡Æ§‡Æ±‡Øç‡Æï‡ØÅ ‡ÆÆ‡ØÅ‡Æ©‡Øç ‡Æé‡Æ™‡Øç‡Æ™‡Øã‡Æ§‡ØÅ‡ÆÆ‡Øç ‡Æ™‡Æ≤ ‡Æ®‡ÆÆ‡Øç‡Æ™‡Æï‡ÆÆ‡Ææ‡Æ© ‡ÆÜ‡Æ§‡Ææ‡Æ∞‡Æô‡Øç‡Æï‡Æ≥‡Æø‡Æ≤‡Æø‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡ØÅ ‡Æö‡Æ∞‡Æø‡Æ™‡Ææ‡Æ∞‡Øç‡Æï‡Øç‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç‡•§",
            'recommendation_fake': "‡Æá‡Æ®‡Øç‡Æ§ ‡Æâ‡Æ≥‡Øç‡Æ≥‡Æü‡Æï‡Øç‡Æï‡Æ§‡Øç‡Æ§‡ØÅ‡Æü‡Æ©‡Øç ‡Æé‡Æö‡Øç‡Æö‡Æ∞‡Æø‡Æï‡Øç‡Æï‡Øà‡ÆØ‡Ææ‡Æï ‡Æá‡Æ∞‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç. ‡Æ™‡Æï‡Æø‡Æ∞‡Øç‡Æµ‡Æ§‡Æ±‡Øç‡Æï‡ØÅ ‡ÆÆ‡ØÅ‡Æ©‡Øç ‡Æ™‡Æ≤ ‡Æ®‡ÆÆ‡Øç‡Æ™‡Æï‡ÆÆ‡Ææ‡Æ© ‡Æö‡ØÜ‡ÆØ‡Øç‡Æ§‡Æø ‡ÆÜ‡Æ§‡Ææ‡Æ∞‡Æô‡Øç‡Æï‡Æ≥‡Æø‡Æ≤‡Æø‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡ØÅ ‡Æö‡Æ∞‡Æø‡Æ™‡Ææ‡Æ∞‡Øç‡Æï‡Øç‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç‡•§",
            'debunked_by': ["ML ‡ÆÆ‡Ææ‡Æ§‡Æø‡Æ∞‡Æø ‡Æµ‡Æï‡Øà‡Æ™‡Øç‡Æ™‡Ææ‡Æü‡ØÅ"],
            'summary': f"ML ‡ÆÆ‡Ææ‡Æ§‡Æø‡Æ∞‡Æø {confidence}% ‡Æ®‡ÆÆ‡Øç‡Æ™‡Æø‡Æï‡Øç‡Æï‡Øà‡ÆØ‡ØÅ‡Æü‡Æ©‡Øç {'‡Æ™‡Øã‡Æ≤‡Æø' if is_fake else '‡Æâ‡Æ£‡Øç‡ÆÆ‡Øà'} ‡Æé‡Æ© ‡Æµ‡Æï‡Øà‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡Æø‡ÆØ‡Æ§‡ØÅ",
            'model_name': "‡Æ™‡ÆØ‡Æø‡Æ±‡Øç‡Æö‡Æø ‡Æ™‡ØÜ‡Æ±‡Øç‡Æ± Kaggle ‡ÆÆ‡Ææ‡Æ§‡Æø‡Æ∞‡Æø"
        },
        'te': {
            'sources': [
                {"name": "ML ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞µ‡∞ø‡∞∂‡±ç‡∞≤‡±á‡∞∑‡∞£", "credibility": "‡∞Ö‡∞ß‡∞ø‡∞ï"},
                {"name": "TF-IDF ‡∞µ‡±Ü‡∞ï‡±ç‡∞ü‡∞∞‡±à‡∞ú‡±á‡∞∑‡∞®‡±ç", "credibility": "‡∞Ö‡∞ß‡∞ø‡∞ï"}
            ],
            'red_flags': [
                "‡∞Ö‡∞®‡±Å‡∞Æ‡∞æ‡∞®‡∞æ‡∞∏‡±ç‡∞™‡∞¶ ‡∞≠‡∞æ‡∞∑‡∞æ ‡∞®‡∞Æ‡±Ç‡∞®‡∞æ‡∞≤‡±Å ‡∞ï‡∞®‡±Å‡∞ó‡±ä‡∞®‡∞¨‡∞°‡±ç‡∞°‡∞æ‡∞Ø‡∞ø",
                "‡∞ï‡∞Ç‡∞ü‡±Ü‡∞Ç‡∞ü‡±ç ‡∞≤‡∞ï‡±ç‡∞∑‡∞£‡∞æ‡∞≤‡±Å ‡∞´‡±á‡∞ï‡±ç ‡∞®‡±ç‡∞Ø‡±Ç‡∞∏‡±ç ‡∞ü‡±ç‡∞∞‡±à‡∞®‡∞ø‡∞Ç‡∞ó‡±ç ‡∞°‡±á‡∞ü‡∞æ‡∞§‡±ã ‡∞∏‡∞∞‡∞ø‡∞™‡±ã‡∞≤‡±Å‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡∞ø",
                "‡∞§‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞Ç ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞Ö‡∞ß‡∞ø‡∞ï ‡∞∏‡∞Ç‡∞≠‡∞æ‡∞µ‡±ç‡∞Ø‡∞§"
            ],
            'recommendation_real': "‡∞ï‡∞Ç‡∞ü‡±Ü‡∞Ç‡∞ü‡±ç ‡∞µ‡∞ø‡∞∂‡±ç‡∞µ‡∞∏‡∞®‡±Ä‡∞Ø‡∞Ç‡∞ó‡∞æ ‡∞ï‡∞®‡∞ø‡∞™‡∞ø‡∞∏‡±ç‡∞§‡±Å‡∞Ç‡∞¶‡∞ø, ‡∞ï‡∞æ‡∞®‡±Ä ‡∞≠‡∞æ‡∞ó‡∞∏‡±ç‡∞µ‡∞æ‡∞Æ‡±ç‡∞Ø‡∞Ç ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å ‡∞é‡∞≤‡±ç‡∞≤‡∞™‡±ç‡∞™‡±Å‡∞°‡±Ç ‡∞Ö‡∞®‡±á‡∞ï ‡∞µ‡∞ø‡∞∂‡±ç‡∞µ‡∞∏‡∞®‡±Ä‡∞Ø ‡∞Æ‡±Ç‡∞≤‡∞æ‡∞≤ ‡∞®‡±Å‡∞Ç‡∞°‡∞ø ‡∞ß‡±É‡∞µ‡±Ä‡∞ï‡∞∞‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø‡•§",
            'recommendation_fake': "‡∞à ‡∞ï‡∞Ç‡∞ü‡±Ü‡∞Ç‡∞ü‡±ç‚Äå‡∞§‡±ã ‡∞ú‡∞æ‡∞ó‡±ç‡∞∞‡∞§‡±ç‡∞§‡∞ó‡∞æ ‡∞â‡∞Ç‡∞°‡∞Ç‡∞°‡∞ø‡•§ ‡∞≠‡∞æ‡∞ó‡∞∏‡±ç‡∞µ‡∞æ‡∞Æ‡±ç‡∞Ø‡∞Ç ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å ‡∞Ö‡∞®‡±á‡∞ï ‡∞µ‡∞ø‡∞∂‡±ç‡∞µ‡∞∏‡∞®‡±Ä‡∞Ø ‡∞µ‡∞æ‡∞∞‡±ç‡∞§‡∞æ ‡∞Æ‡±Ç‡∞≤‡∞æ‡∞≤ ‡∞¶‡±ç‡∞µ‡∞æ‡∞∞‡∞æ ‡∞ß‡±É‡∞µ‡±Ä‡∞ï‡∞∞‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø‡•§",
            'debunked_by': ["ML ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞µ‡∞∞‡±ç‡∞ó‡±Ä‡∞ï‡∞∞‡∞£"],
            'summary': f"ML ‡∞Æ‡±ã‡∞°‡∞≤‡±ç {confidence}% ‡∞µ‡∞ø‡∞∂‡±ç‡∞µ‡∞æ‡∞∏‡∞Ç‡∞§‡±ã {'‡∞´‡±á‡∞ï‡±ç' if is_fake else '‡∞∞‡∞ø‡∞Ø‡∞≤‡±ç'} ‡∞ó‡∞æ ‡∞µ‡∞∞‡±ç‡∞ó‡±Ä‡∞ï‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø‡∞Ç‡∞¶‡∞ø",
            'model_name': "‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞™‡±ä‡∞Ç‡∞¶‡∞ø‡∞® Kaggle ‡∞Æ‡±ã‡∞°‡∞≤‡±ç"
        },
        'en': {
            'sources': [
                {"name": "ML Model Analysis", "credibility": "High"},
                {"name": "TF-IDF Vectorization", "credibility": "High"}
            ],
            'red_flags': [
                "Suspicious language patterns detected",
                "Content characteristics match fake news training data",
                "High probability of misinformation"
            ],
            'recommendation_real': "Content appears credible, but always cross-check with multiple sources.",
            'recommendation_fake': "Always verify news through multiple reliable sources before sharing.",
            'debunked_by': ["ML Model Classification"],
            'summary': f"ML Model classified as {'FAKE' if is_fake else 'REAL'} with {confidence}% confidence",
            'model_name': "Trained Kaggle Model"
        }
    }

    # Get content for the specified language, fallback to English
    lang_content = content.get(language, content['en'])

    return {
        'sources': lang_content['sources'],
        'red_flags': lang_content['red_flags'],
        'recommendation': lang_content['recommendation_fake'] if is_fake else lang_content['recommendation_real'],
        'debunked_by': lang_content['debunked_by'],
        'summary': lang_content['summary'],
        'model_name': lang_content['model_name']
    }

def create_prompt(text, has_image=False, language='en'):
    language_name = LANGUAGES.get(language, 'English')

    return f"""You are a multilingual fact-checking expert AI trained to detect misinformation and verify news claims across different languages and cultures.

IMPORTANT: Respond in {language_name} language. All text fields in the JSON response must be in {language_name}.

{'MULTIMODAL ANALYSIS: Examine both the text content and any visual elements in the image for authenticity, manipulation, misleading context, or out-of-context usage.' if has_image else ''}

Given the news statement below, do the following:
1. Use your internal knowledge and reasoning to determine if the claim is TRUE or FALSE
2. Cross-check it with scientific consensus, verified public statements, and established facts
3. Check if this claim has been previously debunked by fact-checkers globally
4. Analyze language patterns for sensationalism or manipulation tactics
5. Consider cultural context and regional misinformation patterns

REQUIRED JSON RESPONSE FORMAT (all text in {language_name}):
{{
  "isReal": boolean,
  "confidence": number (70-95),
  "reasoning": "Detailed explanation in {language_name} based on factual evidence, scientific consensus, or lack thereof. If previously debunked, mention that clearly (2-3 sentences)",
  "sources": [
    {{"name": "Credible Source Name in {language_name}", "credibility": "High|Medium|Low"}}
  ],
  "redFlags": ["specific warning signs in {language_name}"],
  "factualClaims": ["verifiable claims in {language_name}"],
  "recommendation": "Clear actionable advice for readers in {language_name}",
  "debunkedBy": ["fact-checking organizations if applicable"],
  "language": "{language}",
  "summary": "Brief one-sentence summary in {language_name}"
}}

FACT-CHECKING CRITERIA:
üü¢ REAL NEWS INDICATORS:
- Verifiable through multiple credible sources
- Consistent with scientific consensus
- Reported by established news organizations
- Contains specific, checkable details
- No history of being debunked

üî¥ FAKE NEWS INDICATORS:
- Contradicts established scientific facts
- Uses sensational or fear-inducing language
- Lacks credible source attribution
- Has been debunked by fact-checkers
- Contains impossible or implausible claims
- Designed to provoke emotional reactions

CONFIDENCE LEVELS:
- 90-95%: Definitive evidence (scientifically proven/debunked)
- 80-89%: Strong evidence from multiple credible sources
- 70-79%: Moderate confidence with some uncertainty

{'IMAGE VERIFICATION: Check for signs of digital manipulation, misleading captions, reverse image search indicators, or images taken out of context.' if has_image else ''}

News to analyze: "{text or 'Analyze the uploaded image content'}"

Remember: Provide ONLY the JSON response with all text fields in {language_name}, no additional text."""

@app.route('/')
def index():
    # Add cache busting to force browser refresh
    import time
    cache_buster = str(int(time.time()))
    return render_template('index.html', cache_buster=cache_buster)

@app.route('/api/analyze', methods=['POST'])
def analyze():
    try:
        text = request.form.get('text', '')
        image_file = request.files.get('image')
        language = request.form.get('language', 'en')

        if not text and not image_file:
            return jsonify({'error': 'Either text or image must be provided'}), 400

        # Extract text from image if provided
        extracted_text = ""
        if image_file:
            try:
                image = Image.open(image_file.stream)
                extracted_text = extract_text_from_image(image)
                print(f"Extracted text from image: {extracted_text[:100]}...")
            except Exception as e:
                print(f"Image processing error: {e}")

        # Combine text sources
        analysis_text = text
        if extracted_text:
            analysis_text = f"{text} {extracted_text}".strip()

        if not analysis_text:
            return jsonify({'error': 'No text found to analyze'}), 400

        print(f"Analyzing text: {analysis_text[:100]}...")

        # Step 1: Use ML model for prediction
        is_fake, ml_confidence = predict_fake_news(analysis_text)

        if is_fake is None:
            return jsonify({
                'success': False,
                'error': 'ML model prediction failed'
            }), 500

        # Step 2: Get Gemini explanation
        explanation = get_gemini_explanation(analysis_text, is_fake, ml_confidence, language)

        # Step 3: Create response in the expected format
        confidence_percentage = int(ml_confidence * 100)

        # Multilingual content based on language
        multilingual_content = get_multilingual_content(language, is_fake, confidence_percentage)

        # Generate sources based on prediction and language
        sources = multilingual_content["sources"]

        # Generate red flags for fake news in the target language
        red_flags = multilingual_content["red_flags"] if is_fake else []

        # Create result
        result = {
            "isReal": not is_fake,
            "confidence": confidence_percentage,
            "reasoning": explanation,  # This is now in the target language from Gemini
            "sources": sources,
            "redFlags": red_flags,
            "factualClaims": [analysis_text[:200] + "..." if len(analysis_text) > 200 else analysis_text],
            "recommendation": multilingual_content["recommendation"],
            "debunkedBy": multilingual_content["debunked_by"] if is_fake else [],
            "language": language,
            "summary": multilingual_content["summary"],
            "mlPrediction": {
                "isFake": is_fake,
                "confidence": ml_confidence,
                "model": multilingual_content["model_name"]
            }
        }

        return jsonify({
            'success': True,
            'data': result
        })

    except Exception as e:
        print(f"Analysis error: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/text-to-speech', methods=['POST'])
def text_to_speech():
    try:
        data = request.get_json()
        text = data.get('text', '')
        language = data.get('language', 'en')

        if not text:
            return jsonify({'error': 'Text is required'}), 400

        # Language mapping for gTTS (some languages need specific codes)
        tts_lang_map = {
            'hi': 'hi',    # Hindi
            'ta': 'ta',    # Tamil
            'te': 'te',    # Telugu
            'ml': 'ml',    # Malayalam
            'kn': 'kn',    # Kannada
            'bn': 'bn',    # Bengali
            'gu': 'gu',    # Gujarati
            'mr': 'mr',    # Marathi
            'pa': 'pa',    # Punjabi
            'en': 'en',    # English
            'es': 'es',    # Spanish
            'fr': 'fr',    # French
            'de': 'de',    # German
            'it': 'it',    # Italian
            'pt': 'pt',    # Portuguese
            'ru': 'ru',    # Russian
            'ja': 'ja',    # Japanese
            'ko': 'ko',    # Korean
            'zh': 'zh-cn', # Chinese (Simplified)
            'ar': 'ar',    # Arabic
            'tr': 'tr',    # Turkish
            'nl': 'nl',    # Dutch
            'sv': 'sv'     # Swedish
        }

        # Get the correct TTS language code
        tts_language = tts_lang_map.get(language, 'en')

        print(f"TTS Request - Text: {text[:50]}..., Language: {language} -> {tts_language}")

        # Create TTS audio with error handling
        try:
            print(f"Creating TTS with language: {tts_language}")
            tts = gTTS(text=text, lang=tts_language, slow=False)
            print("TTS object created successfully")
        except Exception as tts_error:
            print(f"TTS Error for language {tts_language}: {tts_error}")
            # Fallback to English if the language is not supported
            print("Falling back to English...")
            tts_language = 'en'
            tts = gTTS(text=text, lang='en', slow=False)

        # Save to temporary file
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as tmp_file:
            print(f"Saving TTS to temporary file: {tmp_file.name}")
            tts.save(tmp_file.name)
            print("TTS saved successfully")

            # Read the file and encode as base64
            with open(tmp_file.name, 'rb') as audio_file:
                audio_data = base64.b64encode(audio_file.read()).decode('utf-8')
                print(f"Audio data encoded, size: {len(audio_data)} characters")

        # Clean up temp file
        try:
            os.unlink(tmp_file.name)
            print("Temporary file cleaned up")
        except Exception as cleanup_error:
            print(f"Warning: Could not clean up temp file: {cleanup_error}")

        return jsonify({
            'success': True,
            'audio': audio_data,
            'format': 'mp3',
            'language': tts_language
        })

    except Exception as e:
        print(f"TTS Error: {str(e)}")
        return jsonify({
            'success': False,
            'error': f'Text-to-speech failed: {str(e)}'
        }), 500

@app.route('/api/languages')
def get_languages():
    return jsonify({
        'success': True,
        'languages': LANGUAGES
    })

@app.route('/api/health')
def health():
    return jsonify({'status': 'OK', 'service': 'TruthLens AI'})

@app.route('/api/test-prediction', methods=['POST'])
def test_prediction():
    """Test endpoint to debug ML model predictions"""
    try:
        data = request.get_json()
        text = data.get('text', '')

        if not text:
            return jsonify({'error': 'Text is required'}), 400

        # Test the ML prediction
        is_fake, confidence = predict_fake_news(text)

        return jsonify({
            'success': True,
            'text': text,
            'prediction': {
                'is_fake': is_fake,
                'confidence': confidence,
                'status': 'FAKE' if is_fake else 'REAL'
            },
            'model_available': fake_news_model is not None,
            'vectorizer_available': tfidf_vectorizer is not None
        })

    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

if __name__ == '__main__':
    import os
    port = int(os.environ.get('PORT', 5001))
    app.run(debug=False, host='0.0.0.0', port=port)
